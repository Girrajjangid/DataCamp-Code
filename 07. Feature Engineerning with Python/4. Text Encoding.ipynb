{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8,12,13,14,15,16,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "so_survey_df = pd.read_csv('../../DataSets/stackoverflow 2018/survey_results_public.csv').iloc[:5000,:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardize your text\n",
    "# remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a-zA-Z] # all letter characters\n",
    "[^a-zA-Z] # all non letter characters\n",
    "\n",
    "df.str.replace('[^a-zA-Z]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the cases\n",
    "df.str.lower()\n",
    "\n",
    "# Length of text to create features from text\n",
    "df.str.len() # this count length of each line\n",
    "\n",
    "# word count\n",
    "df.str.split()\n",
    "\n",
    "# count words\n",
    "df.str.split().str.len()\n",
    "\n",
    "\n",
    "# Average length of word\n",
    "df['charcount'] / df['wordcount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "print(speech_df['text_clean'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of each text\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "\n",
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)\n",
    "\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify arguements to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=.2,max_df=.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with these features\n",
    "cv_df = pd.DataFrame(cv_array, columns = cv.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Representation Term frequency inverse documentation frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf = (count of word occurence / total word in document) / log(number of doc word is in / total number of doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "`\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features = 100 , stop_words = 'english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
    "                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0,:]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
    "                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words and N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(max_features=100, \n",
    "                                 stop_words='english', \n",
    "                                 ngram_range=(3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "print(cv_trigram_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the features\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \n",
    "                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "print(cv_tri_df.sum().sort_values(ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "\n",
    "1. how to understand your data\n",
    "2. efficient encoding or categorical features\n",
    "3. different way to work with continue data\n",
    "\n",
    "# Chapter 2\n",
    "\n",
    "1. How to locate gaps in your data\n",
    "2. best practices in dealing with the incomplete rows\n",
    "3. methods to find and deal with unwanted characters\n",
    "\n",
    "# Chapter 3\n",
    "\n",
    "1. Visualize distributions\n",
    "2. Modify distributions\n",
    "3. Find outliers and remove them\n",
    "\n",
    "# Chapter 4\n",
    "\n",
    "1. Tf-idf\n",
    "2. N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
