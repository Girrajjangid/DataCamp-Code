{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data =   pd.read_csv('../../DataSets/NumericCategorical/TestData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability Test\n",
    "1. Flesh reading ease\n",
    "2. Gunning fog index\n",
    "3. Simple Measure of Gabbledygook (SMOG)\n",
    "4. Dale - chall score\n",
    "\n",
    "library textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textatistic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5d518ca67647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextatistic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextatistic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textatistic'"
     ]
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "readability_score = Textatistic(text).scores  # readabiblity is a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing techniques\n",
    "1. Convert words into lowercase\n",
    "2. Removing trailing and whitespaces\n",
    "3. Removing puntuations\n",
    "4. Removing stopwords\n",
    "5. Expanding contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization  and Lemmatization\n",
    "\n",
    "Tokenization :  Tokenization is the task of chopping sentences into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
    "\n",
    "Lemmatization : Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .  ( remove grammer from word ) like is,am,are = be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list:  ['today', \"'s\", 'weather', 'is', 'am', 'are', 'so', 'hot', 'that', 'I', 'ai', \"n't\", 'go', 'outsided', '.']\n",
      "original String:  [\"today's\", 'weather', 'is', 'am', 'are', 'so', 'hot', 'that', 'I', \"ain't\", 'go', 'outsided.']\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = \"today's weather is am are so hot that I ain't go outsided.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(s) # doc look like a string but it is spacy.token \n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print('token list: ' ,tokens)\n",
    "# Here is a difference all punctuattions are seperated\n",
    "print('original String: ' ,s.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list:  ['today', \"'s\", 'weather', 'be', 'be', 'be', 'so', 'hot', 'that', '-PRON-', 'be', 'not', 'go', 'outsided', '.']\n",
      "original String:  [\"today's\", 'weather', 'is', 'am', 'are', 'so', 'hot', 'that', 'I', \"ain't\", 'go', 'outsided.']\n",
      "today 's weather be be be so hot that -PRON- be not go outsided .\n",
      "today's weather is am are so hot that I ain't go outsided.\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = \"today's weather is am are so hot that I ain't go outsided.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(s) # doc look like a string but it is spacy.token \n",
    "\n",
    "# Generate the tokens\n",
    "lemmas = [token.lemma_ for token in doc]  # lemma_ gives all words in there base form\n",
    "print('token list: ' ,tokens)\n",
    "# Here is a difference all punctuattions are seperated\n",
    "print('original String: ' ,s.split())\n",
    "\n",
    "print(' '.join(lemmas))\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "1. Un-nessary whitespace\n",
    "2. puntuations\n",
    "3. special characters\n",
    "4. Stopwards\n",
    "5. Removing HTML/XML Tags\n",
    "6. Replacing accented characters (such as e')\n",
    "7. Correcting spelling errors\n",
    "8. A word of caution\n",
    "\n",
    "REMEMBER that always remove only those techniques which need for an application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today weather hot outsided\n"
     ]
    }
   ],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(s)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f75a106bc95e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Apply preprocess to ted['transcript']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcript'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcript'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcript'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ted' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging\n",
    "\n",
    "Applications:\n",
    "\n",
    "1. Word sense disambiguation\n",
    "2. Sentiment analysis\n",
    "3. question answering\n",
    "4. Fake news and opinion spam detections\n",
    "\n",
    "\n",
    "POS tagging\n",
    "assigning every word, its corresponding part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('today', 'NOUN')\n",
      "(\"'s\", 'PART')\n",
      "('weather', 'NOUN')\n",
      "('is', 'VERB')\n",
      "('am', 'VERB')\n",
      "('are', 'VERB')\n",
      "('so', 'ADV')\n",
      "('hot', 'ADJ')\n",
      "('that', 'ADP')\n",
      "('I', 'PRON')\n",
      "('ai', 'VERB')\n",
      "(\"n't\", 'ADV')\n",
      "('go', 'VERB')\n",
      "('outsided', 'ADJ')\n",
      "('.', 'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(s)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "for i in pos:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model = nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "     # return pos.count(\"NOUN\")\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. Efficient search algorithms\n",
    "2. Question answering\n",
    "3. News article classfication\n",
    "4. Customer service\n",
    "\n",
    "Identify and classifying named entities into predefined categories.\n",
    "\n",
    "Categories include person, organisation,country, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sundar Pichai']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
