{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building steps using keras\n",
    "\n",
    "1. Specify architecture\n",
    "2. Compile\n",
    "3. Fit\n",
    "4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression problem\n",
    "## Model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = np.loadtxt('data.csv' , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1] # inpput neurons\n",
    "model = Sequential()\n",
    "model.add(Dense(100 , activation='relu' , input_shape = (n_cols,)))\n",
    "model.add(Dense(100 , activation='relu')) # dense means all previous N connect with next layer all N\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile\n",
    "1. Specify the optimizer\n",
    "    \n",
    "    1.1. Controls the learning rate\n",
    "    \n",
    "    1.2. Many options and mathematical complex\n",
    "    \n",
    "    1.3.'Adam' is usually a good choice\n",
    "    \n",
    "\n",
    "2. Loss Function\n",
    " \n",
    "     2.1. \"mean_square_error\" for common regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting\n",
    "\n",
    "1. Applying backpropagation with data and gradient decent to update the weights\n",
    "\n",
    "2. Scaling data before fitting can easy optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "loss = 'categorical_crossentropy\n",
    "\n",
    "similar to log loss:lower is better\n",
    "\n",
    "add metric=['accuracy'] to compile step for easy-to-understand diagnostics\n",
    "\n",
    "output layer has seperate node for each possible outcome and uses 'softmax' activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "predictors = data.drop(['labels'],axis=1).as_matrix()\n",
    "target = to_categorical(data['labels'])\n",
    "model = Sequential()\n",
    "model.add(Dense(100 , activation='relu' , input_shape = (n_cols,)))\n",
    "model.add(Dense(100 , activation='relu'))\n",
    "model.add(Dense(100 , activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving , reloading and using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('my_model.h5') # bi class classfication model  \n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "probability_true = predictions[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Model optimization\n",
    "\n",
    "## why optimization is hard?\n",
    "1. Simultaneously optimizing 1000s of parameters with complex relationships\n",
    "2. updates may not improve model meaningfully\n",
    "3. update too small if learning rate is low and updat too high if learning rate is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100 , activation='relu' , input_shape = input_shape))\n",
    "    model.add(Dense(100 , activation='relu'))\n",
    "    model.add(Dense(2   , activation='softmax'))\n",
    "    return model\n",
    "lr_to_test=[.0000001 , .001 , .01 , .1 , 1]\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr)\n",
    "    model.compile(optimizer = my_optimizer , loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dying neuron problem\n",
    "\n",
    "once a node starts always getting negative inputs\n",
    "\n",
    "It may continue only getting negative inputs\n",
    "\n",
    "Contributes nothing to the model\n",
    "\n",
    "'dead' neuron\n",
    "\n",
    "instead use 'tanh function'\n",
    "\n",
    "## Vanishing gradient\n",
    "\n",
    "Occurs when many layers have very small slopes (e.g. due to being on flat part of tanh curve)\n",
    "\n",
    "In deep network, update to backprop close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "1. Commonly use validation split rather than cross-validation\n",
    "\n",
    "2. Deep learning widely used on large datasets\n",
    "\n",
    "3. Single validation score is based on large amount of data, and is reliable\n",
    "\n",
    "4. repeating training from cross-validation would take long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictors, target, validation_split = 0.3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(pred,target,validation_split=0.3,epochs=20,callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(2 , activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model_1 with 10,10 nueorn \n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working flow of optimizing model capacity\n",
    "\n",
    "1. start with a small network\n",
    "\n",
    "2. Get the validation score\n",
    "\n",
    "3. Keep increasing capacity until validation score is no longer improving (calculate MSE)\n",
    "\n",
    "4. increase layer increase nodes until score is no longer imporoving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
